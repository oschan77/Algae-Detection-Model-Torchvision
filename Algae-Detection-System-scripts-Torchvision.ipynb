{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwsafrPl-9Gv"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "git clone https://github.com/pytorch/vision.git\n",
        "cd vision\n",
        "cp references/detection/coco_eval.py ../\n",
        "cp references/detection/coco_utils.py ../\n",
        "cp references/detection/utils.py ../\n",
        "cp references/detection/transforms.py ../\n",
        "cp references/detection/engine.py ../"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1MWMUniFWIS",
        "outputId": "eea034e8-1b4f-4614-d21c-fb680c76cb02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting data_setup.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile data_setup.py\n",
        "import torch\n",
        "from torchvision.datasets import CocoDetection\n",
        "from torchvision.transforms import transforms\n",
        "\n",
        "class CocoDataset(CocoDetection):\n",
        "  def __init__(self, root, annFile, transform=None, target_transform=None) -> None:\n",
        "    super().__init__(root, annFile, transform, target_transform)\n",
        "\n",
        "  def __getitem__(self, index: int):\n",
        "    img, ori_target = super().__getitem__(index)\n",
        "\n",
        "    num_objs = len(ori_target)\n",
        "    boxes = []\n",
        "    labels = []\n",
        "    area = []\n",
        "    iscrowd = []\n",
        "\n",
        "    for i in range(num_objs):\n",
        "      x_min = max(0, ori_target[i]['bbox'][0])\n",
        "      y_min = max(0, ori_target[i]['bbox'][1])\n",
        "      x_max = min(4908, x_min + ori_target[i]['bbox'][2])\n",
        "      y_max = min(3264, y_min + ori_target[i]['bbox'][3])\n",
        "      boxes.append([x_min, y_min, x_max, y_max])\n",
        "      labels.append(ori_target[i]['category_id'])\n",
        "      area.append(ori_target[i]['area'])\n",
        "      iscrowd.append(ori_target[i]['iscrowd'])\n",
        "\n",
        "    target = {}\n",
        "    target['boxes'] = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "    target['labels'] = torch.as_tensor(labels, dtype=torch.int64)\n",
        "    target['image_id'] = torch.tensor([ori_target[0]['image_id']])\n",
        "    target['area'] = torch.as_tensor(area, dtype=torch.float32)\n",
        "    target['iscrowd'] = torch.as_tensor(iscrowd, dtype=torch.int64)\n",
        "\n",
        "    return img, target\n",
        "\n",
        "\n",
        "def get_transform(train):\n",
        "  if train:\n",
        "    return transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.RandomHorizontalFlip(0.5),\n",
        "    ])\n",
        "  else:\n",
        "    return transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "def split_dataset(dataset_full, train_ratio):\n",
        "  num_train = int(len(dataset_full) * train_ratio)\n",
        "  num_test = len(dataset_full) - num_train\n",
        "\n",
        "  dataset_train, dataset_test = torch.utils.data.random_split(dataset_full, [num_train, num_test])\n",
        "\n",
        "  print(f'Size of the train set: {len(dataset_train)}')\n",
        "  print(f'Size of the test set: {len(dataset_test)}')\n",
        "\n",
        "  return dataset_train, dataset_test\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "  return tuple(zip(*batch))\n",
        "\n",
        "\n",
        "def create_datasets(\n",
        "  root: str,\n",
        "  annFile: str,\n",
        "  train_ratio: float,\n",
        "):\n",
        "  dataset_full = CocoDataset(\n",
        "    root = root,\n",
        "    annFile = annFile,\n",
        "    transform = get_transform(train=True),\n",
        "  )\n",
        "\n",
        "  dataset_train, dataset_test = split_dataset(dataset_full, train_ratio=train_ratio)\n",
        "\n",
        "  return dataset_train, dataset_test\n",
        "\n",
        "\n",
        "def create_dataloaders(\n",
        "  dataset_train: torch.utils.data.Dataset,\n",
        "  dataset_test: torch.utils.data.Dataset,\n",
        "  batch_size: int,\n",
        "  num_workers: int,\n",
        "):\n",
        "  train_dataloader = torch.utils.data.DataLoader(\n",
        "    dataset_train,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = True,\n",
        "    num_workers = num_workers,\n",
        "    collate_fn = collate_fn,\n",
        "  )\n",
        "\n",
        "  test_dataloader = torch.utils.data.DataLoader(\n",
        "    dataset_test,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = False,\n",
        "    num_workers = num_workers,\n",
        "    collate_fn = collate_fn,\n",
        "  )\n",
        "\n",
        "  return train_dataloader, test_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YCNyj8MsUni",
        "outputId": "9acd1bb6-dbf1-4df2-b515-55db8ac934a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting model_builder.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile model_builder.py\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "def get_FasterRCNN_model(num_classes, feature_extract=True):\n",
        "  weights = torchvision.models.detection.FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1\n",
        "  model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(weights=weights)\n",
        "\n",
        "  if feature_extract:\n",
        "    for param in model.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "  in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "  model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRZO8mGPXQJr",
        "outputId": "1af55406-7242-45c8-fec5-3c4d9b8874d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting custom_utils.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile custom_utils.py\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torchvision.transforms import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "def apply_nms(orig_prediction, iou_thresh=0.3):\n",
        "  keep = torchvision.ops.nms(orig_prediction['boxes'].cpu(), orig_prediction['scores'].cpu(), iou_thresh)\n",
        "  \n",
        "  final_prediction = orig_prediction\n",
        "  final_prediction['boxes'] = final_prediction['boxes'].cpu()[keep]\n",
        "  final_prediction['scores'] = final_prediction['scores'].cpu()[keep]\n",
        "  final_prediction['labels'] = final_prediction['labels'].cpu()[keep]\n",
        "  \n",
        "  return final_prediction\n",
        "\n",
        "\n",
        "def torch_to_pil(img):\n",
        "  return transforms.ToPILImage()(img).convert('RGB')\n",
        "\n",
        "\n",
        "def plot_img_bbox(img, target, num_classes):\n",
        "  fig, a = plt.subplots(1, 1)\n",
        "  fig.set_size_inches(10, 10)\n",
        "  a.imshow(img)\n",
        "\n",
        "  for i in range(len(target['boxes'])):\n",
        "    box = target['boxes'][i]\n",
        "    label = int(target['labels'][i])\n",
        "\n",
        "    cmap = plt.cm.get_cmap('hsv', num_classes+1)\n",
        "\n",
        "    x, y, width, height  = box[0], box[1], box[2]-box[0], box[3]-box[1]\n",
        "\n",
        "    rect = patches.Rectangle(\n",
        "      (x, y),\n",
        "      width, height,\n",
        "      linewidth = 2,\n",
        "      edgecolor = cmap(label),\n",
        "      facecolor = 'none'\n",
        "    )\n",
        "\n",
        "    a.add_patch(rect)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def inference_and_plot(\n",
        "  dataset: torch.utils.data.Dataset,\n",
        "  model: nn.Module,\n",
        "  device: str,\n",
        "  iou_thresh: float,\n",
        "  num_classes: int,\n",
        "):\n",
        "  random_idx = random.randint(0, len(dataset)-1)\n",
        "  img, target = dataset[random_idx]\n",
        "\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    prediction = model([img.to(device)])[0]\n",
        "\n",
        "  nms_prediction = apply_nms(prediction, iou_thresh=iou_thresh)\n",
        "\n",
        "  plot_img_bbox(torch_to_pil(img), nms_prediction, num_classes)\n",
        "\n",
        "\n",
        "def save_model(\n",
        "  model: torch.nn.Module,\n",
        "  target_path: str,\n",
        "  model_name: str,\n",
        "):\n",
        "  assert model_name.endswith('pth') or model_name.endswith('.pt'), \"[Invalid model name]: model_name should end with '.pth' or '.pt'.\"\n",
        "\n",
        "  target_path = Path(target_path)\n",
        "  target_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "  torch.save(\n",
        "    obj = model.state_dict(),\n",
        "    f = target_path / model_name,\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkas5XIfK3eL",
        "outputId": "e31a9c01-af89-4fd3-eec4-673137d36f4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting train.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile train.py\n",
        "import argparse\n",
        "import torch\n",
        "from data_setup import create_datasets, create_dataloaders\n",
        "from model_builder import get_FasterRCNN_model\n",
        "from custom_utils import save_model\n",
        "from engine import train_one_epoch, evaluate\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--root', type=str, required=True)\n",
        "parser.add_argument('--annfile', type=str, required=True)\n",
        "parser.add_argument('--target_path', type=str, default='saved_models')\n",
        "parser.add_argument('--model_name', type=str, default='faster_rcnn_v1.pth')\n",
        "parser.add_argument('--num_classes', type=int, default=5)\n",
        "parser.add_argument('--train_ratio', type=float, default=0.8)\n",
        "parser.add_argument('--batch_size', type=int, default=2)\n",
        "parser.add_argument('--num_workers', type=int, default=1)\n",
        "parser.add_argument('--print_freq', type=int, default=1)\n",
        "parser.add_argument('--lr', type=float, default=1e-3)\n",
        "parser.add_argument('--epochs', type=int, default=5)\n",
        "parser.add_argument('--fe', type=bool, default=True)\n",
        "args = parser.parse_args()\n",
        "\n",
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "ROOT = args.root\n",
        "ANNFILE = args.annfile\n",
        "TARGET_PATH = args.target_path\n",
        "MODEL_NAME = args.model_name\n",
        "NUM_CLASSES = args.num_classes\n",
        "TRAIN_RATIO = args.train_ratio\n",
        "BATCH_SIZE = args.batch_size\n",
        "NUM_WORKERS = args.num_workers\n",
        "PRINT_FREQ = args.print_freq\n",
        "LEARNING_RATE = args.lr\n",
        "EPOCHS = args.epochs\n",
        "FEATURE_EXTRACT = args.fe\n",
        "\n",
        "dataset_train, dataset_test = create_datasets(\n",
        "  root = ROOT,\n",
        "  annFile = ANNFILE,\n",
        "  train_ratio = TRAIN_RATIO,\n",
        ")\n",
        "\n",
        "train_dataloader, test_dataloader = create_dataloaders(\n",
        "  dataset_train = dataset_train,\n",
        "  dataset_test = dataset_test,\n",
        "  batch_size = BATCH_SIZE,\n",
        "  num_workers = NUM_WORKERS,\n",
        ")\n",
        "\n",
        "model = get_FasterRCNN_model(num_classes=NUM_CLASSES, feature_extract=FEATURE_EXTRACT)\n",
        "model.to(DEVICE)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  train_one_epoch(model, optimizer, train_dataloader, DEVICE, epoch, print_freq=PRINT_FREQ)\n",
        "  evaluate(model, test_dataloader, device=DEVICE)\n",
        "\n",
        "save_model(\n",
        "  model=model,\n",
        "  target_path=TARGET_PATH,\n",
        "  model_name=MODEL_NAME,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-NmA1V930_O",
        "outputId": "5fc94589-ddfd-462e-ef52-331ba5b7c57b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=1.39s)\n",
            "creating index...\n",
            "index created!\n",
            "Size of the train set: 3\n",
            "Size of the test set: 1\n",
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_v2_coco-dd69338a.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_v2_coco-dd69338a.pth\n",
            "100% 167M/167M [00:03<00:00, 44.7MB/s]\n",
            "Epoch: [0]  [0/2]  eta: 0:00:21  lr: 0.001000  loss: 5.2356 (5.2356)  loss_classifier: 1.5061 (1.5061)  loss_box_reg: 0.1950 (0.1950)  loss_objectness: 3.1654 (3.1654)  loss_rpn_box_reg: 0.3691 (0.3691)  time: 10.9347  data: 4.2959  max mem: 1416\n",
            "Epoch: [0]  [1/2]  eta: 0:00:05  lr: 0.001000  loss: 2.2653 (3.7504)  loss_classifier: 1.5061 (1.5085)  loss_box_reg: 0.1950 (0.2171)  loss_objectness: 0.4963 (1.8308)  loss_rpn_box_reg: 0.0188 (0.1940)  time: 5.5897  data: 2.1727  max mem: 1416\n",
            "Epoch: [0] Total time: 0:00:11 (5.6006 s / it)\n",
            "Test:  [0/1]  eta: 0:00:01  model_time: 0.1510 (0.1510)  evaluator_time: 0.0080 (0.0080)  time: 1.7537  data: 1.5475  max mem: 1416\n",
            "Test: Total time: 0:00:01 (1.7836 s / it)\n",
            "Averaged stats: model_time: 0.1510 (0.1510)  evaluator_time: 0.0080 (0.0080)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.01s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            "Epoch: [1]  [0/2]  eta: 0:00:05  lr: 0.001000  loss: 2.2664 (2.2664)  loss_classifier: 1.1784 (1.1784)  loss_box_reg: 0.2868 (0.2868)  loss_objectness: 0.7499 (0.7499)  loss_rpn_box_reg: 0.0513 (0.0513)  time: 2.7312  data: 2.1381  max mem: 1416\n",
            "Epoch: [1]  [1/2]  eta: 0:00:01  lr: 0.001000  loss: 2.2664 (3.8497)  loss_classifier: 0.8727 (1.0255)  loss_box_reg: 0.0780 (0.1824)  loss_objectness: 0.7499 (2.4594)  loss_rpn_box_reg: 0.0513 (0.1824)  time: 1.7832  data: 1.3873  max mem: 1416\n",
            "Epoch: [1] Total time: 0:00:03 (1.8076 s / it)\n",
            "Test:  [0/1]  eta: 0:00:01  model_time: 0.1343 (0.1343)  evaluator_time: 0.0050 (0.0050)  time: 1.3481  data: 1.1584  max mem: 1416\n",
            "Test: Total time: 0:00:01 (1.3779 s / it)\n",
            "Averaged stats: model_time: 0.1343 (0.1343)  evaluator_time: 0.0050 (0.0050)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.01s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.026\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.065\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.049\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.067\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.200\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.267\n",
            "Epoch: [2]  [0/2]  eta: 0:00:04  lr: 0.001000  loss: 4.0585 (4.0585)  loss_classifier: 0.6250 (0.6250)  loss_box_reg: 0.0478 (0.0478)  loss_objectness: 3.1568 (3.1568)  loss_rpn_box_reg: 0.2289 (0.2289)  time: 2.4106  data: 1.9504  max mem: 1416\n",
            "Epoch: [2]  [1/2]  eta: 0:00:01  lr: 0.001000  loss: 2.2351 (3.1468)  loss_classifier: 0.6250 (0.6484)  loss_box_reg: 0.0478 (0.2390)  loss_objectness: 1.0380 (2.0974)  loss_rpn_box_reg: 0.0951 (0.1620)  time: 1.4209  data: 1.0959  max mem: 1416\n",
            "Epoch: [2] Total time: 0:00:02 (1.4365 s / it)\n",
            "Test:  [0/1]  eta: 0:00:01  model_time: 0.1498 (0.1498)  evaluator_time: 0.0069 (0.0069)  time: 1.1205  data: 0.9160  max mem: 1416\n",
            "Test: Total time: 0:00:01 (1.1508 s / it)\n",
            "Averaged stats: model_time: 0.1498 (0.1498)  evaluator_time: 0.0069 (0.0069)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.01s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            "Epoch: [3]  [0/2]  eta: 0:00:04  lr: 0.001000  loss: 4.9006 (4.9006)  loss_classifier: 0.3519 (0.3519)  loss_box_reg: 0.0387 (0.0387)  loss_objectness: 4.0323 (4.0323)  loss_rpn_box_reg: 0.4778 (0.4778)  time: 2.3534  data: 1.8921  max mem: 1416\n",
            "Epoch: [3]  [1/2]  eta: 0:00:01  lr: 0.001000  loss: 2.6697 (3.7852)  loss_classifier: 0.2381 (0.2950)  loss_box_reg: 0.0102 (0.0244)  loss_objectness: 2.2539 (3.1431)  loss_rpn_box_reg: 0.1675 (0.3227)  time: 1.4716  data: 1.1444  max mem: 1416\n",
            "Epoch: [3] Total time: 0:00:02 (1.4877 s / it)\n",
            "Test:  [0/1]  eta: 0:00:01  model_time: 0.1511 (0.1511)  evaluator_time: 0.0102 (0.0102)  time: 1.0151  data: 0.8061  max mem: 1416\n",
            "Test: Total time: 0:00:01 (1.0446 s / it)\n",
            "Averaged stats: model_time: 0.1511 (0.1511)  evaluator_time: 0.0102 (0.0102)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.01s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.112\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.281\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.079\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.232\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.050\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.290\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.333\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.383\n",
            "Epoch: [4]  [0/2]  eta: 0:00:05  lr: 0.001000  loss: 4.7584 (4.7584)  loss_classifier: 0.2469 (0.2469)  loss_box_reg: 0.0336 (0.0336)  loss_objectness: 4.0125 (4.0125)  loss_rpn_box_reg: 0.4654 (0.4654)  time: 2.5611  data: 1.9637  max mem: 1416\n",
            "Epoch: [4]  [1/2]  eta: 0:00:01  lr: 0.001000  loss: 1.1001 (2.9293)  loss_classifier: 0.2469 (0.3170)  loss_box_reg: 0.0336 (0.1114)  loss_objectness: 0.5050 (2.2587)  loss_rpn_box_reg: 0.0188 (0.2421)  time: 1.6533  data: 1.2533  max mem: 1416\n",
            "Epoch: [4] Total time: 0:00:03 (1.6737 s / it)\n",
            "Test:  [0/1]  eta: 0:00:01  model_time: 0.1506 (0.1506)  evaluator_time: 0.0111 (0.0111)  time: 1.5216  data: 1.3053  max mem: 1416\n",
            "Test: Total time: 0:00:01 (1.5641 s / it)\n",
            "Averaged stats: model_time: 0.1506 (0.1506)  evaluator_time: 0.0111 (0.0111)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.01s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n"
          ]
        }
      ],
      "source": [
        "!python train.py --root '/content/drive/MyDrive/data/algae_toydata/algae_toydata_1' --annfile '/content/drive/MyDrive/data/algae_toydata/annotations/algae_toydata_1.json'"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
